"""
Module d'int√©gration du chatbot dans le dashboard.
Importe directement les modules du chatbot sans passer par chatbot_app.py
"""
import sys
import os

# Ajouter le chemin du chatbot
# Dans Docker, le volume est mont√© √† /app/../backend/chatbot
chatbot_path = os.path.abspath('/app/../backend/chatbot')
if chatbot_path not in sys.path:
    sys.path.insert(0, chatbot_path)

def render_chatbot():
    """
    Affiche le chatbot en important directement ses d√©pendances.
    """
    import streamlit as st
    
    # V√©rifier que les modules n√©cessaires sont disponibles
    try:
        # Imports des modules du chatbot
        from data_retriever import data_retriever
        from memory_utils import prepare_context_for_sql
        from pdf_generator import detect_pdf_request, generate_professional_pdf
        from sql_generator import sql_generator
        
        import google.generativeai as genai
        from dotenv import load_dotenv
        import plotly.express as px
        import plotly.graph_objects as go
        import pandas as pd
        import numpy as np
        import re
        import json
        from datetime import datetime
        
        # Charger les variables d'environnement
        load_dotenv()
        API_KEY = os.getenv("GEMINI_API_KEY")
        
        if not API_KEY:
            st.error("‚ö†Ô∏è Cl√© API Gemini non trouv√©e. D√©finis GEMINI_API_KEY dans ton fichier .env")
            return
        
        # Configuration API Gemini
        genai.configure(api_key=API_KEY)
        
        # Initialisation du mod√®le
        @st.cache_resource
        def init_gemini_model():
            try:
                model = genai.GenerativeModel('gemini-2.5-flash')
                return model, "gemini-2.5-flash"
            except Exception as e:
                st.warning(f"Erreur avec gemini-2.5-flash: {e}. Fallback vers gemini-pro.")
                try:
                    model = genai.GenerativeModel('gemini-pro')
                    return model, "gemini-pro"
                except Exception as e2:
                    st.error(f"Impossible d'initialiser un mod√®le Gemini: {e2}")
                    return None, None
        
        model, model_name = init_gemini_model()
        
        if model is None:
            st.error("Impossible d'initialiser le mod√®le Gemini")
            return
        
        # Fonctions utilitaires (copi√©es de chatbot_app.py)
        def execute_plotly_code_safely(code: str, data_context: dict) -> tuple:
            """Ex√©cute du code Plotly dans un environnement s√©curis√©."""
            forbidden_patterns = [
                r'\bos\b', r'\bsys\b', r'\bsubprocess\b', r'\beval\b', 
                r'\bexec\b', r'\b__import__\b', r'\bopen\b', r'\bfile\b',
                r'\bcompile\b', r'\bglobals\b', r'\blocals\b'
            ]
            
            for pattern in forbidden_patterns:
                if re.search(pattern, code, re.IGNORECASE):
                    return False, f"Code interdit d√©tect√©: {pattern}"
            
            safe_imports = {
                'plotly': __import__('plotly'),
                'px': px,
                'go': go,
                'pd': pd,
                'np': np,
                'json': json
            }
            
            safe_namespace = {
                '__builtins__': {
                    'range': range, 'len': len, 'str': str, 'int': int, 'float': float,
                    'list': list, 'dict': dict, 'tuple': tuple, 'set': set,
                    'zip': zip, 'enumerate': enumerate, 'min': min, 'max': max,
                    'sum': sum, 'abs': abs, 'round': round, 'sorted': sorted,
                    'reversed': reversed, 'map': map, 'filter': filter,
                    'any': any, 'all': all, 'isinstance': isinstance, 'type': type,
                    'bool': bool, 'True': True, 'False': False, 'None': None,
                },
                **safe_imports,
                **data_context
            }
            
            try:
                exec(code, safe_namespace)
                if 'fig' in safe_namespace:
                    return True, safe_namespace['fig']
                else:
                    return False, "Aucune variable 'fig' trouv√©e"
            except Exception as e:
                return False, f"Erreur d'ex√©cution: {str(e)}"
        
        def extract_code_from_response(text: str) -> str:
            """Extrait le code Python d'une r√©ponse."""
            patterns = [
                r'```python\n(.*?)```',
                r'```python\s+(.*?)```',
                r'```py\n(.*?)```',
                r'```\n(.*?)```',
            ]
            
            code = ""
            for pattern in patterns:
                match = re.search(pattern, text, re.DOTALL)
                if match:
                    code = match.group(1).strip()
                    if 'fig' in code or 'px.' in code or 'go.' in code:
                        break
            
            if not code and ('fig =' in text or 'px.' in text or 'go.' in text):
                code = text.strip()
            
            if not code:
                return ""
            
            lines = code.split('\n')
            cleaned_lines = []
            removed_imports = []
            
            for line in lines:
                line_stripped = line.strip()
                if (line_stripped.startswith('import ') or 
                    line_stripped.startswith('from ') or
                    ('import' in line_stripped and ('plotly' in line_stripped or 'pandas' in line_stripped or 'numpy' in line_stripped))):
                    removed_imports.append(line_stripped)
                    continue
                cleaned_lines.append(line)
            
            cleaned_code = '\n'.join(cleaned_lines).strip()
            
            if removed_imports:
                st.info(f"üßπ {len(removed_imports)} import(s) retir√©s")
            
            return cleaned_code
        
        # Prompt syst√®me (version simplifi√©e)
        SYSTEM_PROMPT = """Tu es un expert en analyse d'√©v√©nements. R√©ponds de mani√®re SYNTH√âTIQUE et RAPIDE.

## R√àGLES ABSOLUES

### 1. PAS DE DONN√âES = PAS DE GRAPHIQUE
Si les donn√©es sont vides: explique pourquoi + propose alternatives

### 2. DONN√âES PR√âSENTES = GRAPHIQUE POSSIBLE
Si donn√©es valides: g√©n√®re le code Python Plotly

### 3. STYLE DE R√âPONSE
Va droit au but, synth√©tise, structure avec tableaux/puces.

## GRAPHIQUES INTERACTIFS

**R√àGLES CODE:**
1. **N'IMPORTE RIEN** - px, go, pd, np, df sont D√âJ√Ä disponibles
2. **PAS DE `import plotly` ou `import pandas`**
3. Variable finale DOIT √™tre `fig`
4. V√©rifie colonnes avec `if 'col' in df.columns`

**TEMPLATE:**
```python
if df.empty or 'col_x' not in df.columns:
    df = pd.DataFrame({'col_x': ['A', 'B'], 'col_y': [10, 20]})

fig = px.bar(df, x='col_x', y='col_y', title='Titre')
fig.update_layout(template='plotly_white')
```

**D√âCISION:** Donn√©es valides ‚Üí code Python | Pas de donn√©es ‚Üí explique + alternatives
"""
        
        # Interface du chatbot
        st.markdown("## üõ°Ô∏è Assistant IA - Gestion d'√âv√©nements & Risques")
        
        col_reset, col_info = st.columns([1, 5])
        with col_reset:
            if st.button("üîÑ R√©initialiser", key="chatbot_reset"):
                if 'chatbot_messages' in st.session_state:
                    st.session_state.chatbot_messages = []
                if 'chatbot_history' in st.session_state:
                    st.session_state.chatbot_history = []
                st.rerun()
        
        with col_info:
            st.markdown(f"*Propuls√© par {model_name}*")
        
        # Initialisation des sessions states
        if "chatbot_messages" not in st.session_state:
            st.session_state.chatbot_messages = []
            welcome_message = """### üëã Assistant √âv√©nements

Je r√©ponds rapidement √† vos questions sur:
- üìã √âv√©nements & incidents
- ‚ö†Ô∏è Risques  
- ‚úÖ Mesures correctives
- üë• Personnes impliqu√©es

**Exemples:**
- "√âv√©nements r√©cents"
- "Risques critiques"
- "Graphique des √©v√©nements par mois"

**Pose ta question !** üöÄ
"""
            st.session_state.chatbot_messages.append({
                "role": "assistant",
                "content": welcome_message
            })
        
        if "chatbot_history" not in st.session_state:
            st.session_state.chatbot_history = []
        
        # Affichage de l'historique
        for message in st.session_state.chatbot_messages:
            with st.chat_message(message["role"]):
                if "content" in message:
                    st.markdown(message["content"])
                if "chart" in message:
                    st.plotly_chart(message["chart"], use_container_width=True)
        
        # Zone de saisie
        if prompt := st.chat_input("Posez votre question sur les √©v√©nements, risques ou mesures..."):
            st.session_state.chatbot_messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # D√©tection PDF
            if detect_pdf_request(prompt):
                with st.chat_message("assistant"):
                    st.markdown("### üìÑ G√©n√©ration du rapport PDF...")
                    
                    if len(st.session_state.chatbot_messages) < 3:
                        response_text = "‚ùå Pas assez de conversation pour g√©n√©rer un rapport. Pose d'abord quelques questions !"
                        st.markdown(response_text)
                        st.session_state.chatbot_messages.append({"role": "assistant", "content": response_text})
                    else:
                        try:
                            pdf_buffer = generate_professional_pdf(st.session_state.chatbot_messages, model)
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            filename = f"Rapport_Evenements_{timestamp}.pdf"
                            
                            response_text = "‚úÖ **Rapport g√©n√©r√© !**\n\nT√©l√©charge-le ci-dessous :"
                            st.markdown(response_text)
                            
                            st.download_button(
                                label="üìÑ T√©l√©charger le rapport",
                                data=pdf_buffer,
                                file_name=filename,
                                mime="application/pdf",
                                use_container_width=True,
                                type="primary"
                            )
                            
                            st.session_state.chatbot_messages.append({"role": "assistant", "content": response_text})
                        except Exception as e:
                            error_msg = f"‚ùå Erreur PDF: {str(e)}"
                            st.error(error_msg)
                            st.session_state.chatbot_messages.append({"role": "assistant", "content": error_msg})
                return
            
            # G√©n√©ration de la r√©ponse normale
            with st.chat_message("assistant"):
                with st.spinner("üîç Analyse en cours..."):
                    prepared_history = prepare_context_for_sql(
                        st.session_state.chatbot_history[-3:],
                        prompt
                    )
                    
                    search_result = data_retriever.search_relevant_data(prompt, prepared_history)
                    schema = data_retriever.get_database_schema()
                    
                    context = search_result.get('context', 'Aucune donn√©e')
                    sql_used = search_result.get('sql_used')
                    success = search_result.get('success', False)
                    row_count = search_result.get('row_count', 0)
                
                if success:
                    st.success(f"‚úÖ {row_count} r√©sultat(s) trouv√©(s)")
                
                with st.spinner("ü§î G√©n√©ration de la r√©ponse..."):
                    full_prompt = f"""{SYSTEM_PROMPT}

## Sch√©ma:
{schema}

## Contexte:
{context}

## Question:
{prompt}
"""
                    
                    try:
                        response = model.generate_content(full_prompt)
                        assistant_response = response.text
                        
                        chart_generated = False
                        plotly_figure = None
                        
                        has_valid_data = (context and context.strip() and 
                                         context != "Aucune donn√©e" and len(context.strip()) > 20)
                        
                        if ("```python" in assistant_response or "```" in assistant_response) and has_valid_data:
                            st.info("üìä G√©n√©ration d'un graphique...")
                            code = extract_code_from_response(assistant_response)
                            
                            if code:
                                df = pd.DataFrame()
                                try:
                                    if context and context != "Aucune donn√©e":
                                        lines = context.strip().split('\n')
                                        data_rows = []
                                        current_row = {}
                                        
                                        for line in lines:
                                            line = line.strip()
                                            if line.startswith('### R√©sultat'):
                                                if current_row:
                                                    data_rows.append(current_row)
                                                current_row = {}
                                            elif line.startswith('- ') and ':' in line:
                                                key_val = line[2:].split(':', 1)
                                                if len(key_val) == 2:
                                                    current_row[key_val[0].strip()] = key_val[1].strip()
                                        
                                        if current_row:
                                            data_rows.append(current_row)
                                        
                                        if data_rows:
                                            df = pd.DataFrame(data_rows)
                                            for col in df.columns:
                                                try:
                                                    df[col] = pd.to_numeric(df[col])
                                                except:
                                                    pass
                                except Exception as e:
                                    st.error(f"‚ùå Erreur parsing: {str(e)}")
                                
                                success_code, result = execute_plotly_code_safely(code, {'df': df})
                                
                                if success_code and hasattr(result, 'to_html'):
                                    st.success("‚úÖ Graphique cr√©√© !")
                                    text_only = re.sub(r'```python.*?```', '', assistant_response, flags=re.DOTALL)
                                    if text_only.strip():
                                        st.markdown(text_only.strip())
                                    st.plotly_chart(result, use_container_width=True)
                                    plotly_figure = result
                                    chart_generated = True
                                else:
                                    st.error(f"‚ùå Erreur: {result}")
                                    st.markdown(assistant_response)
                            else:
                                text_only = re.sub(r'```.*?```', '', assistant_response, flags=re.DOTALL)
                                st.markdown(text_only.strip() if text_only.strip() else assistant_response)
                        else:
                            st.markdown(assistant_response)
                        
                        message_data = {"role": "assistant", "content": assistant_response}
                        if chart_generated and plotly_figure is not None:
                            message_data["chart"] = plotly_figure
                        
                        st.session_state.chatbot_messages.append(message_data)
                        
                        st.session_state.chatbot_history.append({
                            "question": prompt,
                            "sql": sql_used if sql_used else "",
                            "result": context[:800] if context else "",
                            "assistant_response": assistant_response[:300]
                        })
                        
                        if len(st.session_state.chatbot_history) > 5:
                            st.session_state.chatbot_history = st.session_state.chatbot_history[-5:]
                    
                    except Exception as e:
                        error_msg = f"‚ùå Erreur: {str(e)}"
                        st.error(error_msg)
                        st.session_state.chatbot_messages.append({"role": "assistant", "content": error_msg})
        
    except ImportError as e:
        st.error(f"‚ö†Ô∏è Erreur d'importation: {str(e)}")
        st.info("V√©rifiez que tous les modules sont dans /app/../backend/chatbot/")
        st.code(f"sys.path = {sys.path}")
    except Exception as e:
        st.error(f"‚ö†Ô∏è Erreur: {str(e)}")
        import traceback
        st.code(traceback.format_exc())

